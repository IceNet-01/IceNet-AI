# IceNet Transformer Configuration - Small Model
# Optimized for Apple M4 Pro with 24GB unified memory

model:
  type: transformer
  hidden_size: 512
  num_layers: 6
  num_heads: 8
  dropout: 0.1
  vocab_size: 50000
  max_seq_length: 512
  activation: gelu
  use_mixed_precision: true
  compile_model: false

training:
  batch_size: 32
  learning_rate: 0.0001
  weight_decay: 0.01
  epochs: 10
  warmup_steps: 1000
  gradient_clip: 1.0
  optimizer: adamw
  scheduler: cosine
  accumulation_steps: 1
  eval_steps: 500
  save_steps: 1000
  logging_steps: 100
  mixed_precision: fp16
  gradient_checkpointing: false

data:
  train_path: data/train
  val_path: data/val
  num_workers: 4
  prefetch_factor: 2
  pin_memory: true
  shuffle: true

system:
  device: auto  # auto, mps, cpu
  seed: 42
  deterministic: false
  benchmark: true

checkpoint:
  output_dir: checkpoints/transformer_small
  save_total_limit: 3
  save_best_only: false
  metric_for_best: loss

logging:
  log_dir: logs/transformer_small
  use_tensorboard: true
  use_wandb: false
  wandb_project: icenet
  wandb_run_name: transformer_small
