# IceNet Transformer Configuration - Large Model
# Requires Apple M4 Pro with 48GB+ unified memory

model:
  type: transformer
  hidden_size: 1024
  num_layers: 12
  num_heads: 16
  dropout: 0.1
  vocab_size: 50000
  max_seq_length: 1024
  activation: gelu
  use_mixed_precision: true
  compile_model: true

training:
  batch_size: 16  # Reduced for large model
  learning_rate: 0.00005
  weight_decay: 0.01
  epochs: 20
  warmup_steps: 2000
  gradient_clip: 1.0
  optimizer: adamw
  scheduler: cosine
  accumulation_steps: 2  # Effective batch size = 32
  eval_steps: 500
  save_steps: 1000
  logging_steps: 100
  mixed_precision: fp16
  gradient_checkpointing: true  # Save memory

data:
  train_path: data/train
  val_path: data/val
  num_workers: 4
  prefetch_factor: 2
  pin_memory: true
  shuffle: true

system:
  device: auto
  seed: 42
  deterministic: false
  benchmark: true

checkpoint:
  output_dir: checkpoints/transformer_large
  save_total_limit: 2
  save_best_only: true
  metric_for_best: loss

logging:
  log_dir: logs/transformer_large
  use_tensorboard: true
  use_wandb: false
